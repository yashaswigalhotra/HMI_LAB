{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HMI_Lab_iiitd.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn277oVvSHdk"
      },
      "source": [
        "# **Implementation by Yashaswi Galhotra**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXtScLjhD3mn"
      },
      "source": [
        "To build a model to recognize emotion from speech using the librosa and sklearn libraries and the Ryerson Audio-Visual Database of Emotional Speech and Song dataset.\n",
        "\n",
        "In this proposed solution, I will use the libraries librosa, soundfile, and sklearn to build a model using an MLP Classifier. MLP Classifier is an artifical neural network This will be able to recognize emotion from sound files. We will load the data, extract features from it, then split the dataset into training and testing sets. Then, we’ll initialize an MLP Classifier and train the model. Finally, we’ll calculate the accuracy,precision, recall, F1 score and confusion matrix of our model.\n",
        "\n",
        "I have used **Librosa package** because\n",
        "*   Librosa is a python package for music and audio analysis. \n",
        "*   It provides the building blocks necessary to create music information retrieval systems\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FapX2KymbuBy"
      },
      "source": [
        "# Importing the important libraries required to implement this problem\n",
        "import librosa\n",
        "import soundfile\n",
        "import os, glob, pickle\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEyXTi3IEONs"
      },
      "source": [
        "I have defined a dictionary which contains the number of emotions the dataset contains and a list called \"required_emotions\" which contains the emotions mentioned in the pdf file which are (Calm, Happy,\n",
        "Sad, Angry, Fearful, Disgust, Surprised and Neutral)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw8Ea3EQci1n"
      },
      "source": [
        "# Emotions in the Ryerson Audio-Visual Database of Emotional Speech and Song dataset\n",
        "emotions={\n",
        "  '01':'neutral',\n",
        "  '02':'calm',\n",
        "  '03':'happy',\n",
        "  '04':'sad',\n",
        "  '05':'angry',\n",
        "  '06':'fearful',\n",
        "  '07':'disgust',\n",
        "  '08':'surprised'\n",
        "}\n",
        "# Emotions to observe\n",
        "required_emotions=['calm','happy','fearful','disgust','sad','angry','surprised','neutral']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRXsgz-aE4fR"
      },
      "source": [
        "Defined a function **extract_feature** to extract the mfcc, chroma, and mel features from a sound file. This function takes 4 parameters- the file name and three Boolean parameters for the three features: \n",
        "\n",
        "* **mfcc:** Mel Frequency Cepstral Coefficient,represents the short-term power spectrum of a sound\n",
        "* **chroma:** Pertains to the 12 different pitch classes\n",
        "* **mel:** Mel Spectrogram Frequency\n",
        "\n",
        "After extracting the features opne the sound file with soundfile. Read from it and call it X. Also, get the sample rate. If chroma is True, get the Short-Time Fourier Transform of X.\n",
        "\n",
        "Now I called hstack() function from numpy because it is used to stack the sequence of input arrays horizontally (i.e. column wise) to make a single array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqnruFIWcfcJ"
      },
      "source": [
        "# Extract features (mfcc, chroma, mel) from a audio file\n",
        "def extract_feature(file_name, mfcc, chroma, mel):\n",
        "    with soundfile.SoundFile(file_name) as sound_file:\n",
        "        X = sound_file.read(dtype=\"float32\")\n",
        "        sample_rate=sound_file.samplerate\n",
        "        if chroma:\n",
        "            stft=np.abs(librosa.stft(X))\n",
        "        result=np.array([])\n",
        "        if mfcc:\n",
        "            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
        "            result=np.hstack((result, mfccs))\n",
        "        if chroma:\n",
        "            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
        "            result=np.hstack((result, chroma))\n",
        "        if mel:\n",
        "            mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
        "            result=np.hstack((result, mel))\n",
        "return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SXq-2EwF9SE"
      },
      "source": [
        "After extracting the features, I have loaded the data with a function **load_data()** – this takes in size of the test set as parameter. x and y are empty lists. I used the **glob() function** from the glob module to get all the pathnames for the Ryerson Audio-Visual Database of Emotional Speech and Song dataset. \n",
        "\n",
        "Using our emotions dictionary, this number is turned into an emotion. It makes a call to extract_feature and stores what is returned in ‘feature’. Then, it appends the feature to x and the emotion to y. So, **the list x holds the features and y holds the emotions**. I called the function train_test_split with these, the test size, and a random state value, and return that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U17mCKvYdUlz"
      },
      "source": [
        "# Load the data and extract features for each audio file\n",
        "def load_data(test_size=0.2):\n",
        "    x,y=[],[]\n",
        "    for file in glob.glob(\"D:\\\\audio-video\\\\Actor_*\\\\*.wav\"):\n",
        "        file_name=os.path.basename(file)\n",
        "        emotion=emotions[file_name.split(\"-\")[2]]\n",
        "        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
        "        x.append(feature)\n",
        "        y.append(emotion)\n",
        "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxZy-lRnGT1a"
      },
      "source": [
        "After assigning the emotions to the dataset, I spilted the dataset into training and testing sets. \n",
        "After spliting the datset, I observe the shape of the training and testing datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSKOVfadcmQZ"
      },
      "source": [
        "# Split the dataset\n",
        "x_train,x_test,y_train,y_test=load_data(test_size=0.25)\n",
        "\n",
        "# Get the shape of the training and testing datasets\n",
        "print((x_train.shape[0], x_test.shape[0]))\n",
        "\n",
        "# Get the number of features extracted\n",
        "print(f'Features extracted: {x_train.shape[1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9yz8BhCG_GY"
      },
      "source": [
        "Initialized an **MLP Classifier**. **Multi-layer Perceptron (MLP) is a supervised learning algorithm that learns a function by training on a dataset, where is the number of dimensions for input and is the number of dimensions for output**. This is a Multi-layer Perceptron Classifier; it optimizes the log-loss function using Limited Memory Broyden–Fletcher–Goldfarb–Shanno . Unlike SVM or Naive Bayes, the **MLP Classifier has an internal neural network for the purpose of classification. This is a feedforward ANN model.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymCevf1odqyU"
      },
      "source": [
        "# Initialize the Multi Layer Perceptron Classifier\n",
        "model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYFXBhJTRfLS"
      },
      "source": [
        "After implementing the MLP Classifier we make model fit on training dataset and predict it's application test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VFdvHKldsRG"
      },
      "source": [
        "# Train the model\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "# Predict for the test set\n",
        "y_pred=model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbSiXDDnRf6k"
      },
      "source": [
        "After training and testing the model on dataset we find the **accuracy, f1 score, recall and confusion matrix.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il0tCi5BdyLd"
      },
      "source": [
        "# Calculate the accuracy of our model\n",
        "accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n",
        "# Print the accuracy\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy*100))\n",
        "#Print the recall\n",
        "recall_score(y_true=y_test, y_pred=y_pred, average=None)\n",
        "print(f'Features extracted: {recall_score}')\n",
        "#Print the f1 score\n",
        "print('F1 score:', f1_score(y_test,y_pred,average='weighted'))\n",
        "#Print the confusion matrix\n",
        "print('\\n confussion matrix:\\n',confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}